% Encoding: UTF-8

@InProceedings{Naesseth:2018,
  author    = {Christian Naesseth and Scott Linderman and Rajesh Ranganath and David Blei},
  title     = {Variational Sequential Monte Carlo},
  booktitle = {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  year      = {2018},
  editor    = {Amos Storkey and Fernando Perez-Cruz},
  volume    = {84},
  series    = {Proceedings of Machine Learning Research},
  pages     = {968--977},
  month     = {09--11 Apr},
  publisher = {PMLR},
  abstract  = {Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.},
  file      = {naesseth18a.pdf:http\://proceedings.mlr.press/v84/naesseth18a/naesseth18a.pdf:PDF},
  url       = { http://proceedings.mlr.press/v84/naesseth18a.html },
}

@Unknown{Robert:2021,
  author = {Robert, Christian and Roberts, Gareth},
  title  = {Rao-Blackwellization in the MCMC era},
  month  = {01},
  year   = {2021},
}

@Misc{Burda:2016,
  author        = {Yuri Burda and Roger Grosse and Ruslan Salakhutdinov},
  title         = {Importance Weighted Autoencoders},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1509.00519},
  primaryclass  = {cs.LG},
}

@InProceedings{Lawson:2018,
  author    = {Lawson, Dieterich and Tucker, George and Naesseth, Christian A and Maddison, Chris and Adams, Ryan P and Teh, Yee Whye},
  title     = {Twisted variational sequential monte carlo},
  booktitle = {Third workshop on Bayesian Deep Learning (NeurIPS)},
  year      = {2018},
}

@Misc{Maddison:2017,
  author        = {Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
  title         = {The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1611.00712},
  primaryclass  = {cs.LG},
}

@InProceedings{Potapczynski:2020,
  author    = {Potapczynski, Andres and Loaiza-Ganem, Gabriel and Cunningham, John P},
  title     = {Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  volume    = {33},
  pages     = {12311--12321},
  publisher = {Curran Associates, Inc.},
  url       = {https://proceedings.neurips.cc/paper/2020/file/90c34175923a36ab7a5de4b981c1972f-Paper.pdf},
}

@InProceedings{Jang:2017,
  author    = {Eric Jang and Shixiang Gu and Ben Poole},
  title     = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  year      = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  url       = {https://openreview.net/forum?id=rkE3y85ee},
}

@Article{Jong:1988,
  author    = {Piet De Jong},
  title     = {The Likelihood for a State Space Model},
  journal   = {Biometrika},
  year      = {1988},
  volume    = {75},
  number    = {1},
  pages     = {165--169},
  issn      = {00063444},
  abstract  = {This paper derives an expression for the likelihood for a state space model. The expression can be evaluated with the Kalman filter initialized at a starting state estimate of zero and associated estimation error covariance matrix of zero. Adjustment for initial conditions can be made after filtering. Accordingly, initial conditions can be modelled without filtering implications. In particular initial conditions can be modelled as `diffuse'. The connection between the `diffuse' and concentrated likelihood is also displayed.},
  publisher = {[Oxford University Press, Biometrika Trust]},
  url       = {http://www.jstor.org/stable/2336450},
}

@Article{Kucukelbir:2017,
  author  = {Alp Kucukelbir and Dustin Tran and Rajesh Ranganath and Andrew Gelman and David M. Blei},
  title   = {Automatic Differentiation Variational Inference},
  journal = {Journal of Machine Learning Research},
  year    = {2017},
  volume  = {18},
  number  = {14},
  pages   = {1-45},
  url     = {http://jmlr.org/papers/v18/16-107.html},
}

@Book{Verbeek:2004,
  title     = {A guide to modern econometrics},
  publisher = {John Wiley \& Sons},
  year      = {2004},
  author    = {Verbeek,Marno},
  address   = {Southern Gate, Chichester, West Sussex, England;Hoboken, NJ;},
  edition   = {2nd},
  isbn      = {0470857730;9780470857731;},
  keywords  = {Econometrics; Regression analysis},
  language  = {English},
}

@Comment{jabref-meta: databaseType:bibtex;}
